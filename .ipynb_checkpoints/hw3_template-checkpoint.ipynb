{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T15:43:35.649486Z",
     "start_time": "2019-10-22T15:43:35.500438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/rmsander/anaconda3/envs/6.869/lib/python3.7/site-packages (0.25.3)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/rmsander/anaconda3/envs/6.869/lib/python3.7/site-packages (from pandas) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/rmsander/anaconda3/envs/6.869/lib/python3.7/site-packages (from pandas) (2.8.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/rmsander/anaconda3/envs/6.869/lib/python3.7/site-packages (from pandas) (1.17.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/rmsander/anaconda3/envs/6.869/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "n_users = 6040\n",
    "n_movies = 3952\n",
    "\n",
    "def get_user_data():\n",
    "    return pd.read_csv('movielens/users.dat', header=None, names=['user_id', 'gender', 'age', 'occupation', 'zip_code'], sep='::', engine='python')\n",
    "\n",
    "def get_movie_data():\n",
    "    return pd.read_csv('movielens/movies.dat', header=None, names=['movie_id', 'title', 'genre'], sep='::', engine='python')\n",
    "\n",
    "def get_rating_data():\n",
    "    return pd.read_csv('movielens/ratings.dat', header=None, names=['user_id', 'movie_id', 'rating', 'timestamp'], sep='::', engine='python')\n",
    "\n",
    "def split_train_val_test(ratings, train=0.8, val=0.1):\n",
    "    shuffled = np.random.RandomState(0).permutation(ratings.index)\n",
    "    n_train = int(len(shuffled) * train)\n",
    "    n_val = int(len(shuffled) * val)\n",
    "    i_train, i_val, i_test = shuffled[:n_train], shuffled[n_train: n_train + n_val], shuffled[-n_val:]\n",
    "    return ratings.loc[i_train], ratings.loc[i_val], ratings.loc[i_test]\n",
    "\n",
    "def get_dense_array(ratings_df):\n",
    "    ratings = np.zeros((n_users, n_movies))\n",
    "    ratings[ratings_df['user_id'] - 1, ratings_df['movie_id'] - 1] = ratings_df['rating']\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T15:43:40.079714Z",
     "start_time": "2019-10-22T15:43:36.383505Z"
    }
   },
   "outputs": [],
   "source": [
    "users = get_user_data()\n",
    "movies = get_movie_data()\n",
    "ratings = get_rating_data()\n",
    "train_ratings_df, val_ratings_df, test_ratings_df = split_train_val_test(ratings)\n",
    "train_ratings, val_ratings, test_ratings = get_dense_array(train_ratings_df), get_dense_array(val_ratings_df), get_dense_array(test_ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Singular Value Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code for Singular Value Thresholding \n",
    "\n",
    "# Let's first interpolate our ratings and fill in with mean value\n",
    "mu = np.mean(train_ratings[train_ratings!=0])\n",
    "A = copy.deepcopy(train_ratings)\n",
    "A[A==0] = mu\n",
    "\n",
    "# Now center our data for USVT\n",
    "A_centered = (A-3)/2\n",
    "\n",
    "# Initialize output data structure\n",
    "A_hat = np.zeros(A_centered.shape)\n",
    "\n",
    "# Compute SVD\n",
    "u,s,v = np.linalg.svd(A_centered,full_matrices=False)\n",
    "print(\"Shape of u: {}\".format(u.shape))\n",
    "print(\"Shape of s: {}\".format(np.diag(s).shape))\n",
    "print(\"Shape of v: {}\".format(v.shape))\n",
    "\n",
    "\n",
    "# Compute components of threshold tau\n",
    "n = A_centered.shape[0]\n",
    "p = np.count_nonzero(train_ratings) / (train_ratings.shape[0]*train_ratings.shape[1])\n",
    "\n",
    "# Collect MSE/hyperparam vals\n",
    "MSEs = []\n",
    "taus = []\n",
    "\n",
    "# Pick random hyperparam values to iterate over\n",
    "eta_vals = np.random.random(size=50)\n",
    "eta_vals.sort()\n",
    "\n",
    "# Iterate over hyperparameter values\n",
    "for eta in eta_vals:\n",
    "\n",
    "    print(\"Eta is: {}\".format(eta))\n",
    "    \n",
    "    # Set hyperparameters for Universal SVT\n",
    "    tau = (2+eta)*np.sqrt(n*p)\n",
    "\n",
    "    # Threshold and sum to compute matrix estimate\n",
    "    s_thresh = np.multiply(s,s>tau)\n",
    "    A_hat = u @ np.diag(s_thresh) @ v # Matrix estimate\n",
    "\n",
    "    # Clip values of estimated matrix\n",
    "    A_hat[A_hat < -1] = -1\n",
    "    A_hat[A_hat > 1] = 1\n",
    "\n",
    "    # Set matrix on 1-5 scale, as in original case\n",
    "    A_hat = 2*A_hat + 3\n",
    "\n",
    "    # Now compute MSE for SVT matrix and interpolated matrix\n",
    "    nz_indices = val_ratings!=0 # Binary mask\n",
    "\n",
    "    # Compute MSE for SVT matrix and save value\n",
    "    MSE_SVT = np.mean(np.square(np.subtract(A_hat[nz_indices],val_ratings[nz_indices])))\n",
    "    MSEs.append(MSE_SVT)\n",
    "    taus.append(tau)\n",
    "    \n",
    "# Compute MSE for interpolated matrix\n",
    "MSE_INTERP = np.mean(np.square(np.subtract(mu,val_ratings[nz_indices])))\n",
    "\n",
    "# Now graph results\n",
    "plt.plot(taus,MSEs)\n",
    "plt.xlabel(\"Thresholding Value (Tau)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE of SVT Estimate as a Function of Thresholding Value Tau\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try for arbitrary values of Tau\n",
    "\n",
    "# Let's first interpolate our ratings and fill in with mean value\n",
    "mu = np.mean(train_ratings[train_ratings!=0])\n",
    "A = copy.deepcopy(train_ratings)\n",
    "A[A==0] = mu\n",
    "\n",
    "# Now center our data for USVT\n",
    "A_centered = (A-3)/2\n",
    "\n",
    "# Initialize output data structure\n",
    "A_hat = np.zeros(A_centered.shape)\n",
    "\n",
    "# Compute SVD\n",
    "u,s,v = np.linalg.svd(A_centered,full_matrices=False)\n",
    "print(\"Shape of u: {}\".format(u.shape))\n",
    "print(\"Shape of s: {}\".format(np.diag(s).shape))\n",
    "print(\"Shape of v: {}\".format(v.shape))\n",
    "\n",
    "\n",
    "# Compute components of threshold tau\n",
    "n = A_centered.shape[0]\n",
    "p = np.count_nonzero(train_ratings) / (train_ratings.shape[0]*train_ratings.shape[1])\n",
    "\n",
    "# Collect MSE/hyperparam vals\n",
    "MSEs = []\n",
    "taus = []\n",
    "\n",
    "# Pick random hyperparam values to iterate over\n",
    "eta_vals = np.random.random(size=50)\n",
    "eta_vals.sort()\n",
    "\n",
    "# Iterate over hyperparameter values\n",
    "for tau in [0,0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 25, 100]:\n",
    "\n",
    "    print(\"Eta is: {}\".format(tau))\n",
    "    \n",
    "    # Threshold and sum to compute matrix estimate\n",
    "    s_thresh = np.multiply(s,s>tau)\n",
    "    A_hat = u @ np.diag(s_thresh) @ v # Matrix estimate\n",
    "\n",
    "    # Clip values of estimated matrix\n",
    "    A_hat[A_hat < -1] = -1\n",
    "    A_hat[A_hat > 1] = 1\n",
    "\n",
    "    # Set matrix on 1-5 scale, as in original case\n",
    "    A_hat = 2*A_hat + 3\n",
    "\n",
    "    # Now compute MSE for SVT matrix and interpolated matrix\n",
    "    nz_indices = val_ratings!=0 # Binary mask\n",
    "\n",
    "    # Compute MSE for SVT matrix and save value\n",
    "    MSE_SVT = np.mean(np.square(np.subtract(A_hat[nz_indices],val_ratings[nz_indices])))\n",
    "    MSEs.append(MSE_SVT)\n",
    "    taus.append(tau)\n",
    "    \n",
    "# Compute MSE for interpolated matrix\n",
    "MSE_INTERP = np.mean(np.square(np.subtract(mu,val_ratings[nz_indices])))\n",
    "\n",
    "# Now graph results\n",
    "plt.plot(taus,MSEs)\n",
    "plt.xlabel(\"Thresholding Value (Tau)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE of SVT Estimate as a Function of Thresholding Value Tau\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try for more arbitrary values of Tau\n",
    "\n",
    "# Let's first interpolate our ratings and fill in with mean value\n",
    "mu = np.mean(train_ratings[train_ratings!=0])\n",
    "A = copy.deepcopy(train_ratings)\n",
    "A[A==0] = mu\n",
    "\n",
    "# Now center our data for USVT\n",
    "A_centered = (A-3)/2\n",
    "\n",
    "# Initialize output data structure\n",
    "A_hat = np.zeros(A_centered.shape)\n",
    "\n",
    "# Compute SVD\n",
    "u,s,v = np.linalg.svd(A_centered,full_matrices=False)\n",
    "print(\"Shape of u: {}\".format(u.shape))\n",
    "print(\"Shape of s: {}\".format(np.diag(s).shape))\n",
    "print(\"Shape of v: {}\".format(v.shape))\n",
    "\n",
    "\n",
    "# Compute components of threshold tau\n",
    "n = A_centered.shape[0]\n",
    "p = np.count_nonzero(train_ratings) / (train_ratings.shape[0]*train_ratings.shape[1])\n",
    "\n",
    "# Collect MSE/hyperparam vals\n",
    "MSEs = []\n",
    "taus = []\n",
    "\n",
    "# Pick random hyperparam values to iterate over\n",
    "eta_vals = np.random.random(size=50)\n",
    "eta_vals.sort()\n",
    "\n",
    "# Iterate over hyperparameter values\n",
    "for tau in [2*i for i in range(50)]:\n",
    "\n",
    "    print(\"Tau is: {}\".format(tau))\n",
    "    \n",
    "    # Threshold and sum to compute matrix estimate\n",
    "    s_thresh = np.multiply(s,s>tau)\n",
    "    A_hat = u @ np.diag(s_thresh) @ v # Matrix estimate\n",
    "\n",
    "    # Clip values of estimated matrix\n",
    "    A_hat[A_hat < -1] = -1\n",
    "    A_hat[A_hat > 1] = 1\n",
    "\n",
    "    # Set matrix on 1-5 scale, as in original case\n",
    "    A_hat = 2*A_hat + 3\n",
    "\n",
    "    # Now compute MSE for SVT matrix and interpolated matrix\n",
    "    nz_indices = val_ratings!=0 # Binary mask\n",
    "\n",
    "    # Compute MSE for SVT matrix and save value\n",
    "    MSE_SVT = np.mean(np.square(np.subtract(A_hat[nz_indices],val_ratings[nz_indices])))\n",
    "    MSEs.append(MSE_SVT)\n",
    "    taus.append(tau)\n",
    "    \n",
    "# Compute MSE for interpolated matrix\n",
    "MSE_INTERP = np.mean(np.square(np.subtract(mu,val_ratings[nz_indices])))\n",
    "\n",
    "# Now graph results\n",
    "plt.plot(taus,MSEs)\n",
    "plt.xlabel(\"Thresholding Value (Tau)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE of SVT Estimate as a Function of Thresholding Value Tau\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of u: (6040, 3952)\n",
      "Shape of s: (3952, 3952)\n",
      "Shape of v: (3952, 3952)\n",
      "5.0 1.0\n",
      "5.0 1.0\n",
      "Validation Mean Squared Error using interpolated matrix: 1.2456092571485262\n",
      "Test Mean Squared Error using tau = 25: 1.0033262138317458\n"
     ]
    }
   ],
   "source": [
    "# Now we can compute test MSE for optimal hyperparam\n",
    "\n",
    "# Let's first interpolate our ratings and fill in with mean value\n",
    "mu = np.mean(train_ratings[train_ratings!=0])\n",
    "A = copy.deepcopy(train_ratings)\n",
    "A[A==0] = mu\n",
    "\n",
    "# Now center our data for USVT\n",
    "A_centered = (A-3)/2\n",
    "\n",
    "# Initialize output data structure\n",
    "A_hat = np.zeros(A_centered.shape)\n",
    "\n",
    "# Compute SVD\n",
    "#u,s,v = np.linalg.svd(A_centered,full_matrices=False)\n",
    "print(\"Shape of u: {}\".format(u.shape))\n",
    "print(\"Shape of s: {}\".format(np.diag(s).shape))\n",
    "print(\"Shape of v: {}\".format(v.shape))\n",
    "\n",
    "\n",
    "# Compute components of threshold tau\n",
    "n = A_centered.shape[0]\n",
    "p = np.count_nonzero(train_ratings) / (train_ratings.shape[0]*train_ratings.shape[1])\n",
    "\n",
    "# Set hyperparameter\n",
    "tau = 25\n",
    "    \n",
    "# Threshold and sum to compute matrix estimate\n",
    "s_thresh = np.multiply(s,s>tau)\n",
    "A_hat = u @ np.diag(s_thresh) @ v # Matrix estimate\n",
    "\n",
    "# Clip values of estimated matrix\n",
    "A_hat[A_hat < -1] = -1\n",
    "A_hat[A_hat > 1] = 1\n",
    "\n",
    "# Set matrix on 1-5 scale, as in original case\n",
    "A_hat = 2*A_hat + 3\n",
    "\n",
    "# Sanity check\n",
    "print(np.amax(A_hat), np.amin(A))\n",
    "print(np.amax(test_ratings), np.amin(test_ratings[test_ratings!=0]))\n",
    "\n",
    "# Now compute MSE for SVT matrix and interpolated matrix\n",
    "nz_indices = test_ratings!=0 # Binary mask for test ratings\n",
    "\n",
    "# Compute MSE for SVT matrix (test)\n",
    "MSE_SVT = np.mean(np.square(np.subtract(A_hat[nz_indices],test_ratings[nz_indices])))\n",
    "\n",
    "    \n",
    "# Compute MSE for interpolated matrix (test)\n",
    "MSE_INTERP = np.mean(np.square(np.subtract(mu,test_ratings[nz_indices])))\n",
    "\n",
    "print(\"Validation Mean Squared Error using interpolated matrix: {}\".format(MSE_INTERP))\n",
    "print(\"Test Mean Squared Error using tau = {}: {}\".format(tau, MSE_SVT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Alternating Least Squares\n",
    "\n",
    "# First, set hyperparameter value k and max_epochs\n",
    "k = 5\n",
    "max_epochs = 10000\n",
    "\n",
    "# Get shape of matrix we're trying to estimate\n",
    "n,m = train_ratings.shape\n",
    "\n",
    "# First, we need to initialize U and V\n",
    "U = np.random.random(size=((n,k)))\n",
    "V = np.random.random(size=((m,k)))\n",
    "\n",
    "# Now we iterate until U and V converge\n",
    "for epoch in range(max_epochs):\n",
    "    # First, perform gradient update\n",
    "    loss = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Code for Collaborative Filtering\n",
    "def cosine_similarity(a, b, ratings):\n",
    "    ra, rb = ratings[a], ratings[b]\n",
    "    # Code to get common ids\n",
    "    a_ids = list(ra.keys())\n",
    "    b_ids = list(rb.keys())\n",
    "\n",
    "    a_set = set(list(ra.keys()))\n",
    "    b_set = set(list(rb.keys()))\n",
    "    \n",
    "    common_ids = list(a_set.intersection(b_set))\n",
    "    \n",
    "    mu_a = np.mean([ra[common_ids[i]] for i in range(len(common_ids))])\n",
    "    mu_b = np.mean([rb[common_ids[i]] for i in range(len(common_ids))])\n",
    "    \n",
    "    ra_p = np.array([ra[common_ids[i]] for i in range(len(common_ids))]) - mu_a\n",
    "    rb_p = np.array([rb[common_ids[i]] for i in range(len(common_ids))]) - mu_b\n",
    "\n",
    "    # Get cosine angle\n",
    "    nz_check = len(np.nonzero(np.linalg.norm(ra_p)*np.linalg.norm(rb_p))[0])\n",
    "    if nz_check:\n",
    "        return np.dot(ra_p,rb_p)/(np.linalg.norm(ra_p)*np.linalg.norm(rb_p))\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "\n",
    "def make_rating_dict(ratings):\n",
    "    # Get matrix of ratings such that R[a] maps to a dictionary of {id: R_a(id)}.\n",
    "    user_ids = ratings[\"user_id\"].to_numpy()\n",
    "    movie_ids = ratings[\"movie_id\"].to_numpy()\n",
    "    rating_ids = ratings[\"rating\"].to_numpy()\n",
    "    user_set = list(set(user_ids))\n",
    "    R = {user_set[i]:{} for i in range(len(user_set))}\n",
    "\n",
    "    counter = 0\n",
    "    for user_id, movie_id, rating_id in zip(user_ids, movie_ids, rating_ids):\n",
    "        if counter % 100000 == 0:\n",
    "            print(\"Iterated through {} ratings\".format(counter))\n",
    "        R[user_id][movie_id] = rating_id\n",
    "        counter += 1\n",
    "    return R, user_set\n",
    "\n",
    "def make_similarity_matrix(R, user_set):\n",
    "    pair_counter = 0\n",
    "    sim = np.zeros((len(user_set),len(user_set)))\n",
    "    for a in user_set:\n",
    "        for b in user_set: # Users are most similar to themselves\n",
    "            if a == b:\n",
    "                pass\n",
    "            if pair_counter % 1000000 == 0:\n",
    "                print(\"Iterated through {} pairs of users\".format(pair_counter))\n",
    "            sim[a-1][b-1] = cosine_similarity(a, b, R)\n",
    "            pair_counter += 1\n",
    "    np.save(\"SIM_MATRIX.npy\", sim, allow_pickle=True)\n",
    "    return sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterated through 0 ratings\n",
      "Iterated through 100000 ratings\n",
      "Iterated through 200000 ratings\n",
      "Iterated through 300000 ratings\n",
      "Iterated through 400000 ratings\n",
      "Iterated through 500000 ratings\n",
      "Iterated through 600000 ratings\n",
      "Iterated through 700000 ratings\n",
      "Iterated through 800000 ratings\n",
      "Iterated through 900000 ratings\n",
      "Iterated through 1000000 ratings\n",
      "Iterated through 0 pairs of users\n",
      "Iterated through 1000000 pairs of users\n",
      "Iterated through 2000000 pairs of users\n",
      "Iterated through 3000000 pairs of users\n",
      "Iterated through 4000000 pairs of users\n",
      "Iterated through 5000000 pairs of users\n",
      "Iterated through 6000000 pairs of users\n",
      "Iterated through 7000000 pairs of users\n",
      "Iterated through 8000000 pairs of users\n",
      "Iterated through 9000000 pairs of users\n",
      "Iterated through 10000000 pairs of users\n",
      "Iterated through 11000000 pairs of users\n",
      "Iterated through 12000000 pairs of users\n",
      "Iterated through 13000000 pairs of users\n",
      "Iterated through 14000000 pairs of users\n",
      "Iterated through 15000000 pairs of users\n",
      "Iterated through 16000000 pairs of users\n",
      "Iterated through 17000000 pairs of users\n",
      "Iterated through 18000000 pairs of users\n",
      "Iterated through 19000000 pairs of users\n",
      "Iterated through 20000000 pairs of users\n",
      "Iterated through 21000000 pairs of users\n",
      "Iterated through 22000000 pairs of users\n",
      "Iterated through 23000000 pairs of users\n",
      "Iterated through 24000000 pairs of users\n",
      "Iterated through 25000000 pairs of users\n",
      "Iterated through 26000000 pairs of users\n",
      "Iterated through 27000000 pairs of users\n"
     ]
    }
   ],
   "source": [
    "# Make rating dict and similarity matrix\n",
    "R, user_set = make_rating_dict(ratings)\n",
    "S = make_similarity_matrix(R, user_set)\n",
    "print(\"SIMILARITY MATRIX SAVED AS: SIM_MATRIX.NPY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Now make code for collaborative filtering\n",
    "def collaborative_filtering(train, sim, k=100):\n",
    "    n,m = train.shape\n",
    "    R_hat = np.zeros((n,m))\n",
    "    for user in range(n): # User-specific code\n",
    "        \n",
    "        # Take specific row of matrix for user\n",
    "        user_sim = sim[a]\n",
    "        \n",
    "        # Find top k most similar users to user in question        \n",
    "        k_indices = list(np.argsort(user_sim))[:k]\n",
    "        \n",
    "        # Now we can iterate over movies\n",
    "        for movie in range(m):\n",
    "            if train[user][movie] != 0:\n",
    "                R[hat] = train[user][movie]\n",
    "            # Get indices for similar people who have rated the movie and average\n",
    "            average = np.sum(np.multiply(train[k_indices][movie], \n",
    "                                         train[k_indices][movie] != 0))/np.nonzero(train[k_indices][movie])\n",
    "            R_hat[user][movie] = average\n",
    "            \n",
    "    return R_hat\n",
    "\n",
    "# Function for computing Mean Squared Error\n",
    "def compute_MSE(R_hat,test_ratings):\n",
    "    # Binary mask for test/validation ratings\n",
    "    nz_indices = test_ratings!=0 \n",
    "\n",
    "    # Compute MSE for Collaborative Filtering matrix (validation)\n",
    "    MSE_SVT = np.mean(np.square(np.subtract(R_hat[nz_indices],test_ratings[nz_indices])))\n",
    "    return MSE_SVT\n",
    "\n",
    "##########################\n",
    "#HYPERPARAMS\n",
    "##########################\n",
    "K = [1,2,5,10,15,20,25,40,50,75,100,125,150,200,250,300,400,500,600,700]\n",
    "\n",
    "# Now load train ratings and similarity matrix\n",
    "train = train_ratings\n",
    "val = val_ratings\n",
    "test = test_ratings\n",
    "sim = np.load(\"SIM_MATRIX.npy\")\n",
    "\n",
    "ks = []\n",
    "val_MSEs = []\n",
    "test_MSEs = []\n",
    "\n",
    "# Iterate over different values of our hyperparameter\n",
    "for k in K:\n",
    "    R_hat = collaborative_filtering(train, sim, k)\n",
    "    # Now get validation MSE\n",
    "    val_MSE = compute_MSE(R_hat, val)\n",
    "    test_MSE = compute_MSE(R_hat, test)\n",
    "\n",
    "    # Append values for later plotting\n",
    "    ks.append(k)\n",
    "    val_MSEs.append(val_MSE)\n",
    "    test_MSEs.append(test_MSE)\n",
    "    print(\"k: {}, val_MSE: {}, test_MSE: {}\".format(k, val_MSE, test_MSE))\n",
    "\n",
    "# Print out ks, val_MSEs, test_MSEs\n",
    "print(\"Hyperparameter values are: {}\".format(ks))\n",
    "print(\"Validation errors are: {}\".format(val_MSEs))\n",
    "print(\"Test errors are: {}\".format(test_MSEs))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot validation error\n",
    "plt.plot(ks,val_MSEs)\n",
    "plt.xlabel(\"Number of similar users (k)\")\n",
    "plt.ylabel(\"Validation Error (MSE)\")\n",
    "plt.title(\"Validation Error of Collaborative Filtering as a Function of k\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# Now plot test error\n",
    "plt.plot(ks,test_MSEs)\n",
    "plt.xlabel(\"Number of similar users (k)\")\n",
    "plt.ylabel(\"Test Error (MSE)\")\n",
    "plt.title(\"Test Error of Collaborative Filtering as a Function of k\")\n",
    "plt.show()\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T05:40:30.959317Z",
     "start_time": "2019-10-22T05:40:30.930064Z"
    }
   },
   "source": [
    "# 4.4 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Neural Networks\n",
    "\n",
    "# Import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class LensNet(nn.Module):\n",
    "    def __init__(self, in_params=100, dr=0.5):\n",
    "        super(LensNet, self).__init__()\n",
    "        self.drop = nn.Dropout(dr)\n",
    "        self.fc1 = nn.Linear(in_params, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8,4)\n",
    "        self.fc5 = nn.Linear(4,2)\n",
    "        self.fc6 = nn.Linear(2,1)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        x = torch.tensor(features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Test neural net\n",
    "\"\"\"\n",
    "Net = LensNet(in_params=10, dr=0.5)\n",
    "x = torch.tensor(np.random.randint(0, high=5, size=10))\n",
    "result = Net.forward(x)\n",
    "print(result)\n",
    "\"\"\"\n",
    "\n",
    "# Now get user and movie data\n",
    "user_data = []\n",
    "with open(\"movielens/users.dat\") as f:\n",
    "    for l in enumerate(f):\n",
    "        user_data.append(l[1].split(\"::\"))\n",
    "        \n",
    "movie_data = []\n",
    "with open(\"movielens/movies.dat\", encoding=\"ISO-8859-1\") as f:\n",
    "    for l in enumerate(f):\n",
    "        movie_data.append(l[1].split(\"::\"))\n",
    "\n",
    "    \n",
    "rating_data = []\n",
    "with open(\"movielens/ratings.dat\") as f:\n",
    "    for l in enumerate(f):\n",
    "        rating_data.append(l[1].split(\"::\"))\n",
    "        \n",
    "print(rating_data)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
